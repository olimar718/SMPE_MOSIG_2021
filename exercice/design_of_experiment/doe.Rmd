---
title: "Design of experiment exercise"
author: "Benjamin Cathelineau"
date: "25/12/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# First approach
The first approach is to assume that the underlying function is of the form $f(x1,x2,\dots,x11)=x1\times c1+x2\times c2+\dots+x1\times c11+intercept+noise=y$

In that case, the best approach seems to figure out each of the of coefficient $(c1,c2,\dots,c11, intercept)$, individually.

To do that, my approach was to set to $1$ the coefficient that I wanted to figure out and to $0$ all the others. To figure out the intercept, I set all the coefficients to $0$.



```{sh}
cat first_experiment.csv
```

The idea is to repeat this experiment multiple time to minimize the effect of $noise$ which is assumed to be random and uniform. I ran the experiment 8 times on the website and saved the result as a csv.
```{r}
library(dplyr)
df =read.csv(file = "first_experiment_result.csv")
```


First, we figure out the intercept by using the experiments with all input set to 0.


```{r}
intercept_exp=df
for (name in colnames(df)){
  name= toString(name)
  if(name != "y" && name != "Date"){
    intercept_exp= intercept_exp %>% filter(intercept_exp[[name]]==0)
  }
}
intercept=mean(intercept_exp$y)
intercept
```
Then we figure each individual coefficient by using the intercept:

```{r}
for (name in colnames(df)){
  name= toString(name)
  if(name != "y" && name != "Date"){
    coeff_exp= df %>% filter(df[[name]]==1)
    print(paste(name,": ",mean(coeff_exp$y)-intercept))
  }
}

```

We can use a linear regression for a sanity check and we see that we obtain exactly the same result. This linear regression will also allow us to predict.

```{r}
form <- as.formula(paste("y~", paste0("x", 1:11, collapse="+")))
reg=lm(form,data=df)
reg
```


Now that we have a linear model, we can generate experiments to test it. We simply generate 10 set of 11 random number that we output to a csv file, so that we can copy paste it to the website. We fix a seed to avoid loosing our value by running the code multiple time.
```{r}
set.seed(1)
new_random_experiment=t(runif(n=11))
for (n in  1:9){
  appendable=t(runif(n=11))
  new_random_experiment=rbind(new_random_experiment,appendable)
}
write.table(x=new_random_experiment,file = "first_experiment_test.csv",row.names = FALSE,col.names = FALSE,sep = ",")
```

Before even going to the website, we will now try to use our model to predict $y$.

Here are our experiments:
```{r}
colnames(new_random_experiment) <- c("x1","x2","x3","x4","x5","x6","x7","x8","x9","x10","x11")
new_random_experiment
```

Now let's get the prediction from our model :

```{r}
for (n in 1:10){
  print(predict(object = reg,newdata=as.list(new_random_experiment[n,])))
}
```

Now we can try running the experiment on the website to see if we get similar results. We save the result as a csv file to keep it for later.

```{r}
first_experiment_results=read.csv(file = "first_experiment_test_results.csv")
first_experiment_results
```

We immediately see that most of our prediction are completely of the mark.
We can compare them with the actual result, and make an average of the error:

```{r}
compare_prediction <- function(amount_experiment,model,data,result){

error_sum=0
for (n in 1:amount_experiment){
  prediction=predict(object = model,newdata=as.list(data[n,]))
  error=prediction-result$y[n]
  error_sum=error_sum+  abs(error)
  print(paste("prediction: ",prediction, ", Actual value: ",result$y[n],", Error: ",error))

}
error_sum/10
}
```

```{r}
compare_prediction(amount_experiment = 10,model = reg, data = new_random_experiment,result = first_experiment_results)
```



This is not surprising as, I asked the professor *Arnaud Legrand* on Mattermost and he told me that, while this was a decent first approach, the underlying function was *not* actually of the form $f(x1,x2,\dots,x11)=x1\times c1+x2\times c2+\dots+x1\times c11+intercept+noise=y$

# Second approach

## Eliminate some coefficient
In my opinion we can use the results of the first experiment for the coefficient that have no effect, especially if we remember that *Arnaud Legrand* gave us the hint that some coefficient (ie some inputs) don't really affect the output. In fact, from the original experiment, it seems that only 3 coefficient matters^[in the meaning that they change the output significantly, ie more that $10^{-2}$] ($x1,x4,x9$)

This is risky, because theses coefficient that seem to not change the output ($x2,x3,x5,x6,x7,x8,x10,x11$) according to the first design, may actually change the output in a complex way that we do not yet see. Nonetheless this would greatly increase the efficiency of the experiment if the assumption is correct.

I am willing to take that risk.


First, instead of the completely non randomized design that I had before, I'm now going to use the *Latin Hyper Square* design presented in class. After a lengthy process to install the library, I just slightly modified the code to adapt it to the current problem.
```{r}
library(DoE.wrapper);   set.seed(42);
d5_HD = lhs.design( type= "maximin" , nruns= 100 ,nfactors= 11,
    seed= 42 , factor.names=list( x1=c(0,1),x2=c(0,0),x3=c(0,0),x4=c(0,1),x5=c(0,0),x6=c(0,0),x7=c(0,0),x8=c(0,0),x9=c(0,1),x10=c(0,0),x11=c(0,0) ) )

 
d5_HD[is.na(d5_HD)] <- 0
write.table(x=d5_HD,file = "second_experiment.csv",row.names = FALSE,col.names = FALSE,sep = ",")
```

Now I run those experiment on the website and get the results:

```{r}
library(dplyr)
df =read.csv(file = "second_experiment_results.csv")
```

```{r}
newreg=lm(y~x1+x4+x9,data=df)
newreg
```

I then run the prediction and compare them against the actual value:
```{r}
compare_prediction(amount_experiment = 10,model = newreg,data = new_random_experiment,result = first_experiment_results)
```
The average error is $0.28$. This is isn't perfect but it's a start, and it's $\approx 2$ times better than the previous approach. 

## Remove x4
For some reason, the average error is slightly lower when we remove $x4$.
```{r}
newreg3=lm(y~x1+x9,data=df)
newreg3
```

```{r}
compare_prediction(amount_experiment = 10,model = newreg3,data = new_random_experiment,result = first_experiment_results)
```
## Try with all coefficients

We redo a experiment design, this time with all the coefficient
```{r}
library(DoE.wrapper);   set.seed(42);
d5_HD = lhs.design( type= "maximin" , nruns= 100 ,nfactors= 11,
    seed= 42 , factor.names=list( x1=c(0,1),x2=c(0,1),x3=c(0,1),x4=c(0,1),x5=c(0,1),x6=c(0,1),x7=c(0,1),x8=c(0,1),x9=c(0,1),x10=c(0,1),x11=c(0,1) ) )

 
d5_HD[is.na(d5_HD)] <- 0
write.table(x=d5_HD,file = "third_experiment.csv",row.names = FALSE,col.names = FALSE,sep = ",")
```

```{r}
library(dplyr)
df =read.csv(file = "third_experiment_results.csv")
```
```{r}
plot(df$x9,df$y)
```



```{r}
newreg2=lm(form,data=df)
newreg2
```

```{r}
compare_prediction(amount_experiment = 10,model = newreg2,data = new_random_experiment,result = first_experiment_results)
```
The average error is barely better than the  $x1+x4+x9$ model, but worse than the $x1+x9$ model...

## Step reg model
I remembered the bat exercise and decided to try to use the `step` command.
```{r}
reg0=lm(y~1,data = df)
stepreg= step(reg0,scope = form, direction = "forward")
summary(stepreg)
```

Notice that it does land on a $x9+x4+x1$.

Let's test this model:

```{r}
compare_prediction(amount_experiment = 10,model = stepreg,data = new_random_experiment,result = first_experiment_results)

```
It's worse than the $x1+x9$ model... :(.

## step without x4
$x4$ is suspicious.
```{r}
reg0=lm(y~1,data = df)
stepreg2= step(reg0,scope = y~x1+x2+x3+x5+x6+x7+x8+x9+x10+x11, direction = "forward")
summary(stepreg2)
```

```{r}
compare_prediction(amount_experiment = 10,model = stepreg2,data = new_random_experiment,result = first_experiment_results)

```
Slightly better...

#Third approach : polynomial regression
Inspired by the hints of Arnaud Legrand, I guessed that the output was actually influenced by a polynomial of $x1$. 

```{r}
x1polreg=lm(formula = y ~ x1 + I(x1^2) + I(x1^3),data = df)
plot(df$x1,df$y)
newdat = data.frame(x1 = seq(min(0), max(1), length.out = 100))
newdat$pred = predict(x1polreg, newdata = newdat)

plot(y ~ x1, data = df)
with(newdat, lines(x = x1, y = pred))
```
It looks very good graphically.

Now for the classical test
```{r}
polreg=lm( y ~ x1 + I(x1^2) + I(x1^3)+x4+x9,data = df)
```

```{r}
compare_prediction(amount_experiment = 10,model = polreg,data = new_random_experiment,result = first_experiment_results)
```
This is a bit less than $\approx$ twice better (in term of average error) than the previously best model. Nice !

# Fourth approach : Gaussian process


The goal is now to maximize the value of $y$.
We know^[we can't really be 100 % sure but...] from the previous experiment that only $x1,x4,x9$ 

```{r}
#inspired by https://cran.r-project.org/web/packages/GauPro/vignettes/GauPro.html
library(GauPro)
gp <- GauPro(df$x1,df$y)
plot(df$x1,df$y)
curve(gp$predict(x), add=T, col=2)
curve(gp$predict(x)+2*gp$predict(x, se=T)$se, add=T, col=4)
curve(gp$predict(x)-2*gp$predict(x, se=T)$se, add=T, col=4)
```
First we see that the Gaussian regression is being drawn with our existing data

### Applying the gaussian process from the start

### First iteration
```{r}
library(GauPro)
set.seed(23)
exp=runif(1)
zeroes=",0,0,0,0,0,0,0,0,0,0"
paste(exp,zeroes ,sep = "")
```
*run the experiment on the website*

```{r}

df = data.frame(x1=exp,y=2.01682006893946)
```

```{r}

gp <- GauPro(df$x1,df$y)
plot(df$x1,df$y)
curve(gp$predict(x), add=T, col=2)
curve(gp$predict(x)+2*gp$predict(x, se=T)$se, add=T, col=4)
curve(gp$predict(x)-2*gp$predict(x, se=T)$se, add=T, col=4)
```

### Iteration 2
Still take uniform because
```{r}
exp=runif(1)
paste(exp,zeroes ,sep = "")

```
Run on the website
```{r}

dfnew = data.frame(x1=exp,y=1.10407525260443)
df = rbind(df,dfnew)
```

```{r}

gp <- GauPro(df$x1,df$y)
plot(df$x1,df$y)
curve(gp$predict(x), add=T, col=2)
curve(gp$predict(x)+2*gp$predict(x, se=T)$se, add=T, col=4)
curve(gp$predict(x)-2*gp$predict(x, se=T)$se, add=T, col=4)
```
### Iteration 3
This time we take the visually highest confidence interval (it's hard to tell where it is ), so we follow the algorithm
```{r}
exp=0.4
paste(exp,zeroes ,sep = "")

```
Run on the website
```{r}

dfnew = data.frame(x1=exp,y=1.35960882383993)
df = rbind(df,dfnew)
```

```{r}

gp <- GauPro(df$x1,df$y)
plot(df$x1,df$y)
curve(gp$predict(x), add=T, col=2)
curve(gp$predict(x)+2*gp$predict(x, se=T)$se, add=T, col=4)
curve(gp$predict(x)-2*gp$predict(x, se=T)$se, add=T, col=4)
```
#Iteration 4
This time we explore at 1.0 (just to make sure to see the whole range of input)
```{r}
exp=1.0
paste(exp,zeroes ,sep = "")

```
Run on the website
```{r}

dfnew = data.frame(x1=exp,y=0.423569793078973)
df = rbind(df,dfnew)
```

```{r}

gp <- GauPro(df$x1,df$y)
plot(df$x1,df$y)
curve(gp$predict(x), add=T, col=2)
curve(gp$predict(x)+2*gp$predict(x, se=T)$se, add=T, col=4)
curve(gp$predict(x)-2*gp$predict(x, se=T)$se, add=T, col=4)
```
#Iteration 5
This time we explore at 0.0 (just to make sure to see the whole range of input)
```{r}
exp=0.0
paste(exp,zeroes ,sep = "")

```
Run on the website
```{r}

dfnew = data.frame(x1=exp,y=1.01524582045845)
df = rbind(df,dfnew)
```

```{r}

gp <- GauPro(df$x1,df$y)
plot(df$x1,df$y)
curve(gp$predict(x), add=T, col=2)
curve(gp$predict(x)+2*gp$predict(x, se=T)$se, add=T, col=4)
curve(gp$predict(x)-2*gp$predict(x, se=T)$se, add=T, col=4)
```
#Iteration 6
This time we explore at 0.6 (Following the algorithm by going to the highest confidence interval)
```{r}
exp=0.6
paste(exp,zeroes ,sep = "")

```
Run on the website
```{r}

dfnew = data.frame(x1=exp,y=2.14170035853349)
df = rbind(df,dfnew)
```

```{r}

gp <- GauPro(df$x1,df$y)
plot(df$x1,df$y)
curve(gp$predict(x), add=T, col=2)
curve(gp$predict(x)+2*gp$predict(x, se=T)$se, add=T, col=4)
curve(gp$predict(x)-2*gp$predict(x, se=T)$se, add=T, col=4)
```
The optimal value of $x1$ is around $0.6$

## With the shiny app
With the shiny app I found an optimal value for $x1$ of around $0.74$, $x4$ of around $1$ and $x9$ of $0$
